{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T13:31:24.187961Z",
     "start_time": "2025-01-08T13:31:24.174869Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Define your original dataset directory and the new structure\n",
    "original_dataset_dir = '../.local/datasets/IoT_ObjectDetection'  # Path to your original dataset folder\n",
    "new_dataset_dir = '../.local/datasets/IoT_ObjectDetection_Yolo'  # Path to the new structured dataset"
   ],
   "id": "e00bd5be433555fb",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T13:31:24.963469Z",
     "start_time": "2025-01-08T13:31:24.275900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Collect all class folders\n",
    "class_folders = [d for d in os.listdir(original_dataset_dir) if os.path.isdir(os.path.join(original_dataset_dir, d))]\n",
    "class_folders.sort()  # Sort class folders to maintain consistent class index\n",
    "\n",
    "# write a function that counts the ids for a given folder and removes the ones that are not the most common\n",
    "def remove_uncommon_ids(class_folder):\n",
    "    label_files = glob.glob(os.path.join(class_folder, '*.txt'))\n",
    "\n",
    "    print(f\"Class: {class_folder}, Number of label files: {len(label_files)}\")\n",
    "\n",
    "    # Count the number of different class IDs in the label files\n",
    "    class_ids = []\n",
    "    for label_file in label_files:\n",
    "        with open(label_file, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) > 0:\n",
    "                    class_ids.append(parts[0])\n",
    "\n",
    "    # Get the most common class ID\n",
    "    common_class_id, common_class_count = np.unique(class_ids, return_counts=True)\n",
    "    common_class_id = common_class_id[np.argmax(common_class_count)]\n",
    "    print(f\"Common class ID: {common_class_id}\")\n",
    "\n",
    "    for label_file in label_files:\n",
    "        with open(label_file, 'r') as f:\n",
    "            lines = [line for line in f.readlines()]\n",
    "        new_lines = []\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) > 0 and parts[0] == common_class_id:\n",
    "                new_lines.append(line)\n",
    "            else:\n",
    "                print(f\"Removing line: {line}\")\n",
    "        with open(label_file, 'w') as f:\n",
    "            for line in new_lines:\n",
    "                f.write(line)\n",
    "\n",
    "        # remove txt files that are empty with their corresponding image\n",
    "        if os.path.getsize(label_file) == 0:\n",
    "            os.remove(label_file)\n",
    "            os.remove(label_file.replace('.txt', '.jpg'))\n",
    "            print(f\"Empty label file: {label_file}\")\n",
    "\n",
    "remove_uncommon_ids(original_dataset_dir + '/Leubot')\n",
    "\n",
    "# in each folder iterate over all txt files and count the different IDs\n",
    "for class_name in class_folders:\n",
    "    class_folder = os.path.join(original_dataset_dir, class_name)\n",
    "    label_files = glob.glob(os.path.join(class_folder, '*.txt'))\n",
    "\n",
    "    # Count the number of different class IDs in the label files\n",
    "    class_ids = []\n",
    "    for label_file in label_files:\n",
    "        with open(label_file, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) > 0:\n",
    "                    class_ids.append(parts[0])\n",
    "\n",
    "    print(f\"Class: {class_name}, Number of different class IDs: {np.unique(class_ids, return_counts=True)}\")\n"
   ],
   "id": "ffb9062de84ffd37",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: ../.local/datasets/IoT_ObjectDetection/Leubot, Number of label files: 290\n",
      "Common class ID: 1\n",
      "Class: DeskBulb, Number of different class IDs: (array(['3'], dtype='<U1'), array([61]))\n",
      "Class: HueLamp, Number of different class IDs: (array(['10', '11', '7', '8', '9'], dtype='<U2'), array([132, 215,   1, 112, 164]))\n",
      "Class: HueLampHalf, Number of different class IDs: (array(['3'], dtype='<U1'), array([113]))\n",
      "Class: Leubot, Number of different class IDs: (array(['1'], dtype='<U1'), array([328]))\n",
      "Class: RoboticArm, Number of different class IDs: (array(['0', '1'], dtype='<U1'), array([310, 477]))\n",
      "Class: Tractorbot, Number of different class IDs: (array(['0'], dtype='<U1'), array([677]))\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T13:31:25.213799Z",
     "start_time": "2025-01-08T13:31:25.207839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# read the file Tractorbot_116 (2).txt and print the content\n",
    "with open(original_dataset_dir + '/Tractorbot/Tractorbot_116 (2).txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        print(line)"
   ],
   "id": "b93f37b55fa35c6a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.318981 0.560677 0.180556 0.107813\n",
      "\n",
      "0 0.422685 0.316406 0.173148 0.102604\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T13:31:36.029082Z",
     "start_time": "2025-01-08T13:31:25.244452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create new directories for train and validation\n",
    "os.makedirs(os.path.join(new_dataset_dir, 'train', 'images'), exist_ok=True)\n",
    "os.makedirs(os.path.join(new_dataset_dir, 'train', 'labels'), exist_ok=True)\n",
    "os.makedirs(os.path.join(new_dataset_dir, 'val', 'images'), exist_ok=True)\n",
    "os.makedirs(os.path.join(new_dataset_dir, 'val', 'labels'), exist_ok=True)\n",
    "\n",
    "# Collect all class folders\n",
    "class_folders = [d for d in os.listdir(original_dataset_dir) if os.path.isdir(os.path.join(original_dataset_dir, d))]\n",
    "class_folders.sort()  # Sort class folders to maintain consistent class index\n",
    "\n",
    "# Create a mapping from class name to class index\n",
    "class_mapping = {class_name: idx for idx, class_name in enumerate(class_folders)}\n",
    "print(class_mapping)\n",
    "\n",
    "for class_name in class_folders:\n",
    "    class_folder = os.path.join(original_dataset_dir, class_name)\n",
    "    image_files = glob.glob(os.path.join(class_folder, '*.jpg'))\n",
    "    print(f\"Class: {class_name}, Number of images: {len(image_files)}\")\n",
    "\n",
    "    # Randomly sample 10 images for validation\n",
    "    val_samples = random.sample(image_files, min(10, len(image_files)))\n",
    "\n",
    "    for img_path in image_files:\n",
    "        # Get the base name without extension\n",
    "        base_name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "\n",
    "        # Move the image to the train images folder if it's not in the validation sample\n",
    "        if img_path not in val_samples:\n",
    "            new_img_path = os.path.join(new_dataset_dir, 'train', 'images', os.path.basename(img_path))\n",
    "            shutil.copy(img_path, new_img_path)\n",
    "\n",
    "            # Move the corresponding label file\n",
    "            label_file = os.path.join(class_folder, f\"{base_name}.txt\")\n",
    "            if os.path.exists(label_file):\n",
    "                new_label_path = os.path.join(new_dataset_dir, 'train', 'labels', f\"{base_name}.txt\")\n",
    "                with open(label_file, 'r') as f:\n",
    "                    with open(new_label_path, 'w') as f_out:\n",
    "                        # Adjust the class index in the label file\n",
    "                        for line in f.readlines():\n",
    "                            parts = line.strip().split()\n",
    "                            if len(parts) > 0:\n",
    "                                # Replace the original class index with the correct index from the mapping\n",
    "                                original_class_index = parts[0]\n",
    "                                new_class_index = class_mapping[class_name]\n",
    "                                # Write in YOLO format (new_class_index x_center y_center width height)\n",
    "                                f_out.write(f\"{new_class_index} {' '.join(parts[1:])}\\n\")\n",
    "\n",
    "        else:\n",
    "            # Move the selected validation image\n",
    "            new_img_path = os.path.join(new_dataset_dir, 'val', 'images', os.path.basename(img_path))\n",
    "            shutil.copy(img_path, new_img_path)\n",
    "\n",
    "            # Move the corresponding validation label file\n",
    "            label_file = os.path.join(class_folder, f\"{base_name}.txt\")\n",
    "            if os.path.exists(label_file):\n",
    "                new_label_path = os.path.join(new_dataset_dir, 'val', 'labels', f\"{base_name}.txt\")\n",
    "                with open(label_file, 'r') as f:\n",
    "                    with open(new_label_path, 'w') as f_out:\n",
    "                        lines = [line for line in f.readlines()]\n",
    "                        # Adjust the class index in the label file\n",
    "                        for line in lines:\n",
    "                            parts = line.strip().split()\n",
    "\n",
    "                            if len(parts) > 0:\n",
    "                                # Replace the original class index with the correct index from the mapping\n",
    "                                original_class_index = parts[0]\n",
    "                                new_class_index = class_mapping[class_name]\n",
    "                                # Write in YOLO format (new_class_index x_center y_center width height)\n",
    "                                f_out.write(f\"{new_class_index} {' '.join(parts[1:])}\\n\")\n",
    "\n",
    "print(\"Dataset reorganized with random samples for validation and class indices adjusted!\")\n",
    "\n",
    "dir_path = \"/home/tobias/Desktop/Uni/IMP/datasets/IoT_ObjectDetection_Yolo\"\n",
    "# if the path exists, remove it\n",
    "if os.path.exists(dir_path):\n",
    "    shutil.rmtree(dir_path)\n",
    "\n",
    "# copy the new dataset to the path\n",
    "shutil.copytree(new_dataset_dir, dir_path)"
   ],
   "id": "9c90c3cfc0c75e55",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DeskBulb': 0, 'HueLamp': 1, 'HueLampHalf': 2, 'Leubot': 3, 'RoboticArm': 4, 'Tractorbot': 5}\n",
      "Class: DeskBulb, Number of images: 61\n",
      "Class: HueLamp, Number of images: 623\n",
      "Class: HueLampHalf, Number of images: 113\n",
      "Class: Leubot, Number of images: 290\n",
      "Class: RoboticArm, Number of images: 680\n",
      "Class: Tractorbot, Number of images: 370\n",
      "Dataset reorganized with random samples for validation and class indices adjusted!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/tobias/Desktop/Uni/IMP/datasets/IoT_ObjectDetection_Yolo'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import yaml\n",
    "\n",
    "# Get class names from the folders in the original dataset directory\n",
    "classes = [d for d in os.listdir(original_dataset_dir) if os.path.isdir(os.path.join(original_dataset_dir, d))]\n",
    "classes.sort()\n",
    "\n",
    "data_dict = {\n",
    "    'train': \"../IoT_ObjectDetection_Yolo/train\",\n",
    "    'val': \"../IoT_ObjectDetection_Yolo/val\",\n",
    "    'nc': len(classes),\n",
    "    'names': classes,\n",
    "}\n",
    "print(classes)\n",
    "\n",
    "dataset_yaml_path = '/home/tobias/Desktop/Uni/IMP/IMP_Magical_Gaze-based_Device_Control/.local/datasets/IoT_ObjectDetection_Yolo/data.yaml'\n",
    "# Save the YAML file\n",
    "with open(dataset_yaml_path, 'w') as f:\n",
    "    yaml.dump(data_dict, f)\n",
    "\n",
    "print(\"data.yaml file created!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "TRAINED_MODEL_PATH = '../.local/models/object_detection/yolov11s_trained.pt'\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO('yolo11s.pt')\n",
    "\n",
    "# Train the model\n",
    "model.train(\n",
    "    data=dataset_yaml_path,\n",
    "    epochs=50,\n",
    "    imgsz=640,\n",
    "    batch=16,\n",
    "    augment=True,          # Enable augmentations\n",
    "    hsv_h=0.015,           # Adjust hue variation\n",
    "    hsv_s=0.7,             # Adjust saturation variation\n",
    "    hsv_v=0.4,             # Adjust value (brightness) variation\n",
    "    degrees=10.0,          # Random rotation degrees\n",
    "    translate=0.1,         # Random translation\n",
    "    scale=0.5,             # Scale variation\n",
    "    shear=2.0,             # Shear variation\n",
    "    #mixup=0.1,             # Mixup augmentation\n",
    ")\n",
    "\n",
    "print(\"Training completed!\")"
   ],
   "id": "39ceb12732a68e4d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save the trained model\n",
    "TRAINED_MODEL_PATH = '../.local/models/object_detection/yolov11s_trained.pt'\n",
    "\n",
    "model.save(TRAINED_MODEL_PATH)"
   ],
   "id": "8c01f809a684f81d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(TRAINED_MODEL_PATH)\n",
    "# Perform validation\n",
    "results = model.val(data=dataset_yaml_path, save_json=True)  # Validate and save results in JSON format"
   ],
   "id": "8365233826987776",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import glob\n",
    "import yaml\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO(TRAINED_MODEL_PATH)\n",
    "\n",
    "dataset_dir = \"/home/tobias/Desktop/Uni/IMP/datasets/\"\n",
    "\n",
    "# predict on the validation set and visualize the results\n",
    "with open('data.yaml') as f:\n",
    "    data = yaml.load(f, Loader=yaml.FullLoader)\n",
    "print(data['val'])\n",
    "val_images = glob.glob(os.path.join(dataset_dir, data['val'], 'images', '*.jpg'))\n",
    "\n",
    "for img_path in val_images:\n",
    "    results = model(img_path)\n",
    "    # Process results list\n",
    "    for result in results:\n",
    "        boxes = result.boxes  # Boxes object for bounding box outputs\n",
    "        masks = result.masks  # Masks object for segmentation masks outputs\n",
    "        keypoints = result.keypoints  # Keypoints object for pose outputs\n",
    "        probs = result.probs  # Probs object for classification outputs\n",
    "        obb = result.obb  # Oriented boxes object for OBB outputs\n",
    "        result.show()  # display to screen\n",
    "    "
   ],
   "id": "a74e7ec85ec4f85b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fcd65f05faff9543",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
